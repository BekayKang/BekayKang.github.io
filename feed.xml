<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://bekaykang.github.io</id><title>Bekay</title><subtitle>A minimal, portfolio, sidebar, bootstrap Jekyll theme with responsive web design and focuses on text presentation.</subtitle> <updated>2021-06-17T19:41:28+09:00</updated> <author> <name>Bekay</name> <uri>https://bekaykang.github.io</uri> </author><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bekaykang.github.io" rel="alternate" type="text/html" /> <generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator> <rights> © 2021 Bekay </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>Diagrams - Diagram as Code</title><link href="https://bekaykang.github.io/posts/Diagrams/" rel="alternate" type="text/html" title="Diagrams - Diagram as Code" /><published>2021-06-13T21:25:00+09:00</published> <updated>2021-06-17T19:40:53+09:00</updated> <id>https://bekaykang.github.io/posts/Diagrams/</id> <content src="https://bekaykang.github.io/posts/Diagrams/" /> <author> <name>Bekay</name> </author> <category term="Visualization" /> <summary> Diagrams: Diagram as Code Python Code만으로 Design tools 없이 전문가 처럼 깔끔한 Diagram을 그릴 수 있는 Lib이 있어서 소개한다. Diagrams 기본적으로 AWS,Azure,Kubernetes,Alibaba Cloud,Oracle Cloud의 diagram을 제공하며, Custom도 가능하다. AWS에서 제공하는 Diagram의 정보는 아래 링크에서 확인이 가능하다. Diagram from AWS Example 예시로, Neural Network에서 Inference의 과정을 Diagram으로 표현하였다. 매우 간단하다! Code </summary> </entry> <entry><title>Policy Gradient</title><link href="https://bekaykang.github.io/posts/policy_gradient/" rel="alternate" type="text/html" title="Policy Gradient" /><published>2021-05-16T11:25:00+09:00</published> <updated>2021-05-17T07:17:42+09:00</updated> <id>https://bekaykang.github.io/posts/policy_gradient/</id> <content src="https://bekaykang.github.io/posts/policy_gradient/" /> <author> <name>Bekay</name> </author> <category term="Reinforcement Learning" /> <category term="Policy Gradient" /> <summary> Objective Function Policy gradient의 objective는 결국 exptected return의 maximization! \[\begin{equation} J(\theta_\pi) = \underset{\tau \sim \pi_\theta}{\mathbb{E}} [R(\tau)] \tag{1} \\ \\ \end{equation}\] 만약 objective function $J(\pi_\theta)$의 derivation을 구할 수 잇다면 policy를 gradient ascent 방향으로 업데이트하면 expected return을 maximization하는 poicy로 업데이트할 수 있다. \[\begin{equation} \theta_{k+1} = \theta_k + ... </summary> </entry> <entry><title>Markdown LaTex Math Symbols</title><link href="https://bekaykang.github.io/posts/markdown-latex/" rel="alternate" type="text/html" title="Markdown LaTex Math Symbols" /><published>2021-04-28T11:25:00+09:00</published> <updated>2021-05-16T15:52:21+09:00</updated> <id>https://bekaykang.github.io/posts/markdown-latex/</id> <content src="https://bekaykang.github.io/posts/markdown-latex/" /> <author> <name>Bekay</name> </author> <category term="Github Tip" /> <summary> Markdown LaTex Symbols 논문에서 나오는 수식들을 정리할때 자주 등장하는 symbols 위주로 정리! (내가 볼려고 하는 정리…ㅋㅋㅋ) Symbol Expression Symbol Expression \(\sum_{t=1}^{T}\) $$\sum_{t=1}^{T}$$ \(\int_{t=1}^{T}\) $$\sum_{t=1}^{T}$$ \(\overset{T}{\underset{t=0}{\prod}}\) $$\sum_{t=1}^{T}$$ \(\frac{A}{B}\) $$\sum_{t=1}^{T}$... </summary> </entry> <entry><title>Python - super() 클래스 상속</title><link href="https://bekaykang.github.io/posts/Python-Super/" rel="alternate" type="text/html" title="Python - super() 클래스 상속" /><published>2021-04-15T21:25:00+09:00</published> <updated>2021-04-15T21:25:00+09:00</updated> <id>https://bekaykang.github.io/posts/Python-Super/</id> <content src="https://bekaykang.github.io/posts/Python-Super/" /> <author> <name>Bekay</name> </author> <category term="Python" /> <summary> Python: super() 클래스 상속 :clipboard: Code를 작성하다보면 먼저 작성한 Class를 활용하면 좋을 때가 있다. 예를 들면, 기존에 작성한 Class에서 구현한 함수를 지금 작성하는 Class에서 끌어다가 쓰고싶을 때이다. 이럴떄 사용하는 방법이 클래스 상속 super()이다. 예제를 하나 만들어보면, 먼저 더하기, 빼기, 나누기, 곱하기의 기능을 구현할 수 있는 calculator()라는 정의했다. calculator Class의 기본 기능이 앞으로 작성할 다른 여러 Class에서 사용해야할 때, 작성하는 매 Class마다 더하기,빼기, 나누니, 곱하기의 기능을 새로 작성한다면 매우 비효율적이다.:scream: 따라서 calculator Class를 상속받아 활... </summary> </entry> <entry><title>Upper-Confidence-Bound Action Selection (UCB)</title><link href="https://bekaykang.github.io/posts/RL-2-7/" rel="alternate" type="text/html" title="Upper-Confidence-Bound Action Selection (UCB)" /><published>2021-01-23T09:25:00+09:00</published> <updated>2021-05-16T15:52:21+09:00</updated> <id>https://bekaykang.github.io/posts/RL-2-7/</id> <content src="https://bekaykang.github.io/posts/RL-2-7/" /> <author> <name>Bekay</name> </author> <category term="Reinforcement Learning" /> <category term="RL by Sutton &amp; Barto" /> <summary> Exploration in Reinforcement Learning 강화학습에서는 Exploration(탐험)이 굉장히 중요하다. Action value estimation에는 불확실성이 항상 존재하기 때문에 Exploration을 통해서 궁극적 학습 목표를 달성하고자 한다. ε-greedy method가 Exploration과 Exploitation을 강제적으로 ε의 확률로 balancing하는 것이다. ε-greedy method에서의 Exploration은 선택 가능한 Action 중에서 랜덤하게 하나의 Action을 선택한다. 물론 이 방법도 나쁘지 않다. 하지만 조금 더 영리하고 효과적인 방법이 없을까? Upper-Confidence-Bound (UCB) 예를들어, Explor... </summary> </entry> </feed>
