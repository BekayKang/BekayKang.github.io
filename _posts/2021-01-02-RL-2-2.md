---
title: Action-value Methods
author: Bekay
date: 2021-01-02 08:25:00 +0800
categories: [RL by Sutton & Barto]
tags: [Reinforcement Learning, RL, Action-value Methods, ε-greedy methods]
math: true
image: /assets/img/post/210102-1.png
---


---
## Action-value Methods
---
[**A k-armed Bandit Problem**](https://bekaykang.github.io/posts/RL-2-1/)
에서 우리의 목적은 **Value of the action (Expected total reward)**를 최대화 하는 것이다. 

목적을 달성하기 위해서 우리는 참 값인 **Value of the action**을 모르기 때문에 추정을 통해서 계산한다. 참 값인 **Value of the action**을 $$q_*(a)$$로 denote하고 추정값인 **Value of the action**을 $$Q_t(a)$$로 denote한다. 추정된 **Value of the action**값을 이용하여 action selection decesion을 내리는 것을 **Action-value methods**라고 한다.

그렇다면, **Value of the action**을 어떻게 추정하는지에 대해서 알아보자.
**Value of the action**의 의미를 되짚어보면 특정한 Action은 선택하였을 때 얻는 **Mean reward**이다.

**Mean reward**는 아래와 같이 계산할 수 있다.

<center> $$Q_t(a) \doteq {\sum_{n=1}^{t-1}R_i|A_t=a \over \sum_{n=1}^{t-1}|A_t=a}$$ </center>

즉, 시각 t 이전에 action 'a'를 취해서 얻은 모든 reward의 합 나누기 시각 t 이전에 action 'a'를 취한 횟수. 

$$Q_t(a)$$의 분모가 0일 경우에는 $$Q_t(a)$$를 0과 같은 특정 default 값으로 설정한다. 반대로 $$Q_t(a)$$의 분모가 무한대일 경우에는 결국 무한대만큼의 sampling을 수행한 결과이므로 이는 결국 $$q_*(a)$$에 수렴한다. 이 방법을 통해 **Value of the action**을 추정하는 것을 **Sample-average method**라 한다.

**Sample-average method**는 action values를 추정하기 위한 **하나**의 방법이지, 다른 방법들도 더 많이 있다.

---
## Greedy Action Selection
---
위에서 정의한 $$Q_t(a)$$를 이용하여 선택 가능한 actions들 중에서 가장 높은 추정 값을 선택하는 것이 **Greedy Action Selection**이다.

<center> $$A_t \doteq argmax_(Q_t(a))$$ </center>

$$argmax(Q_t(a))$$를 이용하여 action selection을 수행하게 되면 [**A k-armed Bandit Problem**](https://bekaykang.github.io/posts/RL-2-1/)에서 정의한 Greedy action만 선택하는 **Exploiting**을 한다.

[**A k-armed Bandit Problem**](https://bekaykang.github.io/posts/RL-2-1/)에서 언급한 것 처럼 **Exploiting**과 **Exploring**의 balance를 찾는 것은 강화학습에서 중요한 연구분야 중 하나이다. 

---
## ε-greedy methods
---
**Exploiting**만 수행을 하게되면 long term으로 학습 결과를 기대할 때 높은 결과를 얻지 못 할 수도 있다. 따라서 **Exploring**을 수행하기 위해서 **ε** 만큼의 작은 확률로 randomly하게 action을 선택하게끔 하는 방법이 **ε-greedy methods**이다.

**ε** 만큼의 확률로 randomly하게 action을 선택하게끔 하므로 **Exploring**과 **Exploiting**의 balance를 맞추는 하나의 방법이다.


